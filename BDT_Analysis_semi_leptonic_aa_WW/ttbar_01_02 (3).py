# -*- coding: utf-8 -*-
"""ttbar_01_02.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lVoqESMEbVnX7WOyQI82q1DrBH6073we
"""

pip install uproot

import uproot
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import roc_curve, auc, accuracy_score
import matplotlib.pyplot as plt

from google.colab import files
uploaded = files.upload()

# Open the ROOT file
file_e = uproot.open("mad356_NP1_Bkg_TTree_electron_Variables.root")
file_m = uproot.open("mad356_NP1_Bkg_TTree_muon_Variables.root")

# List all top-level object names and their types
print("Keys in the electron file:", file_e.keys())
print("Class names:", file_e.classnames())
#-------------------------------------------------
print("Keys in the moun file:", file_m.keys())
print("Class names:", file_m.classnames())

# ROOT files
electron_file = "mad356_NP1_Bkg_TTree_electron_Variables.root"
muon_file     = "mad356_NP1_Bkg_TTree_muon_Variables.root"

# Correct tree lists
signal_trees_e = [
    "SM_electron_TTree", "cHj1_electron_TTree", "cHj3_electron_TTree",
    "cHu_electron_TTree", "cHQ3_electron_TTree", "cQj31_electron_TTree",
    "cQj38_electron_TTree", "ctBIm_electron_TTree", "ctWIm_electron_TTree",
    "ctWRe_electron_TTree", "ctBRe_electron_TTree"
]
signal_trees_mu = [
    "SM_muon_TTree", "cHj1_muon_TTree", "cHj3_muon_TTree",
    "cHu_muon_TTree", "cHQ3_muon_TTree", "cQj31_muon_TTree",
    "cQj38_muon_TTree", "ctBIm_muon_TTree", "ctWIm_muon_TTree",
    "ctWRe_muon_TTree", "ctBRe_muon_TTree"
]

bkg_trees_e = [
    "TTbarZ_electron_TTree", "TW_channel_electron_TTree",
    "Single_Top_S_channel_electron_TTree", "TTbar_electron_TTree",
    "WlvJets_electron_TTree", "Single_Top_T_channel_electron_TTree",
    "ZJets_electron_TTree", "WlvWJetsJets_electron_TTree",
    "WZ_electron_TTree", "WWZ_electron_TTree", "TTbarW_electron_TTree"
]
bkg_trees_mu = [
    "TTbarZ_muon_TTree", "TW_channel_muon_TTree",
    "Single_Top_S_channel_muon_TTree", "TTbar_muon_TTree",
    "WlvJets_muon_TTree", "Single_Top_T_channel_muon_TTree",
    "ZJets_muon_TTree", "WlvWJetsJets_muon_TTree",
    "WZ_muon_TTree", "WWZ_muon_TTree", "TTbarW_muon_TTree"
]

all_vars = [
    "Angle_JetBJet",
    "DeltaR_JetBJet",
    "massTransverse_lepBJetMet",
    "massTransverse_lepMet",
    "massT_Top",
    "PtTop",
    "numJet"
]

# Example weights
signal_weight = 0.144679
cHj1_weight  = 0.146588
cHj3_weight  = 0.193391
cHu_weight   = 0.147011
cHQ3_weight  = 0.180315
cQj31_weight = 0.231448
cQj38_weight = 0.184248
ctBIm_weight = 0.147659
ctWIm_weight = 0.15757
ctWRe_weight = 0.156258
ctBRe_weight = 0.14721

TTbarZ_weight                   = 0.0974457
TW_channel_weight               = 48.9888
Single_Top_S_channel_weight     = 4.90839
TTbar_weight                    = 411.092
WlvJets_weight                  = 27725.9
Single_Top_T_channel_weight     = 154.247
ZJets_weight                    = 44400.3
WlvWJetsJets_weight             = 47.5836
WZ_weight                       = 2.43213
WWZ_weight                      = 0.0146152
TTbarW_weight                   = 0.223391

# Build weight lists matching the tree lists
signal_weights = [
    signal_weight, cHj1_weight, cHj3_weight, cHu_weight, cHQ3_weight,
    cQj31_weight, cQj38_weight, ctBIm_weight, ctWIm_weight,
    ctWRe_weight, ctBRe_weight
]
bkg_weights = [
    TTbarZ_weight, TW_channel_weight, Single_Top_S_channel_weight,
    TTbar_weight, WlvJets_weight, Single_Top_T_channel_weight,
    ZJets_weight, WlvWJetsJets_weight, WZ_weight,
    WWZ_weight, TTbarW_weight
]

# Helper to load and concatenate per-tree DataFrames
def load_and_concat(file_path, tree_list, weight_list, label, variables):
    dfs = []
    with uproot.open(file_path) as f:
        for tree_name, w in zip(tree_list, weight_list):
            tree = f[tree_name]
            available = [v for v in variables if v in tree.keys()]
            missing = set(variables) - set(available)
            if missing:
                print(f"  Warning: tree '{tree_name}' missing {missing}")
            arr = tree.arrays(available, library="np")
            df = pd.DataFrame(arr)
            df['label']  = label
            df['weight'] = w
            dfs.append(df)
    return pd.concat(dfs, ignore_index=True)

# Now load each collection correctly
df_signal_e  = load_and_concat(electron_file, signal_trees_e, signal_weights, 1, all_vars)
df_signal_mu = load_and_concat(muon_file,     signal_trees_mu, signal_weights, 1, all_vars)
df_bkg_e     = load_and_concat(electron_file, bkg_trees_e,     bkg_weights,  0, all_vars)
df_bkg_mu    = load_and_concat(muon_file,     bkg_trees_mu,     bkg_weights,  0, all_vars)

# Combine all into one DataFrame
df = pd.concat([df_signal_e, df_signal_mu, df_bkg_e, df_bkg_mu], ignore_index=True)

# Quick sanity checks:
print("Total events:", len(df))
print("Labels distribution:\n", df['label'].value_counts())
print("Weight sum per class:\n",
      df.groupby('label')['weight'].sum())

"""
#import uproot
#import pandas as pd

# ROOT files
electron_file = "mad356_NP1_Bkg_TTree_electron_Variables.root"
muon_file     = "mad356_NP1_Bkg_TTree_muon_Variables.root"

# Correct tree lists
signal_trees_e = ["SM_electron_TTree", "cHj1_electron_TTree", "cHj3_electron_TTree", "cHu_electron_TTree", "cHQ3_electron_TTree", "cQj31_electron_TTree", "cQj38_electron_TTree", "ctBIm_electron_TTree", "ctWIm_electron_TTree", "ctWRe_electron_TTree", "ctBRe_electron_TTree"]
signal_trees_mu = ["SM_muon_TTree", "cHj1_muon_TTree", "cHj3_muon_TTree", "cHu_muon_TTree", "cHQ3_muon_TTree", "cQj31_muon_TTree", "cQj38_muon_TTree", "ctBIm_muon_TTree", "ctWIm_muon_TTree", "ctWRe_muon_TTree", "ctBRe_muon_TTree"]
bkg_trees_e    = ["TTbarZ_electron_TTree", "TW_channel_electron_TTree", "Single_Top_S_channel_electron_TTree", "TTbar_electron_TTree", "WlvJets_electron_TTree", "Single_Top_T_channel_electron_TTree", "ZJets_electron_TTree", "WlvWJetsJets_electron_TTree", "WZ_electron_TTree", "WWZ_electron_TTree", "TTbarW_electron_TTree"]
bkg_trees_mu   = ["TTbarZ_muon_TTree", "TW_channel_muon_TTree", "Single_Top_S_channel_muon_TTree", "TTbar_muon_TTree", "WlvJets_muon_TTree", "Single_Top_T_channel_muon_TTree", "ZJets_muon_TTree", "WlvWJetsJets_muon_TTree", "WZ_muon_TTree", "WWZ_muon_TTree", "TTbarW_muon_TTree"]

# Variables you *intend* to use
all_vars = ["Angle_JetBJet", "DeltaR_JetBJet", "massT_Top", "PtTop", "numJet"]

def load_channel(file_path, trees, label, weight, variables):
    dfs = []
    with uproot.open(file_path) as f:
        for t in trees:
            tree = f[t]
            available = [v for v in variables if v in tree.keys()]
            missing = set(variables) - set(available)
            if missing:
                print(f"  Warning: tree '{t}' missing {missing}")
            arr = tree.arrays(available, library="np")
            df = pd.DataFrame(arr)
            df['label'] = label
            df['weight'] = weight
            dfs.append(df)
    return pd.concat(dfs, ignore_index=True)

# Example weights
signal_weight = 0.144679
cHj1_weight  = 0.146588
cHj3_weight  = 0.193391
cHu_weight  = 0.147011
cHQ3_weight   = 0.180315
cQj31_weight   = 0.231448
cQj38_weight   = 0.184248
ctBIm_weight   = 0.147659
ctWIm_weight   = 0.15757
ctWRe_weight   = 0.156258
ctBRe_weight   = 0.14721


TTbarZ_weight = 0.0974457
TW_channel_weight = 48.9888
Single_Top_S_channel_weight = 4.90839
TTbar_weight = 411.092
WlvJets_weight = 27725.9
Single_Top_T_channel_weight = 154.247
ZJets_weight = 44400.3
WlvWJetsJets_weight = 47.5836
WZ_weight = 2.43213
WWZ_weight = 0.0146152
TTbarW_weight = 0.223391

# Load each sample
df_signal_e = load_channel(electron_file, signal_trees_e, 1, signal_weight, all_vars)
df_signal_mu = load_channel(muon_file, signal_trees_mu, 1, signal_weight, all_vars)
df_bkg_e    = load_channel(electron_file, bkg_trees_e,    0, bkg_weight,    all_vars)
df_bkg_mu   = load_channel(muon_file,    bkg_trees_mu,    0, bkg_weight,    all_vars)

# Combine
df = pd.concat([df_signal_e, df_signal_mu, df_bkg_e, df_bkg_mu], ignore_index=True)
"""

# --- 2. Prepare training and testing datasets ---

# Determine which of the desired variables are actually in the combined DataFrame
available_vars_in_df = [var for var in all_vars if var in df.columns]

# Check if any of the desired variables were not found in the combined DataFrame
missing_vars_in_df = set(all_vars) - set(available_vars_in_df)
if missing_vars_in_df:
    print(f"Warning: The following variables were not found in the combined DataFrame and will be excluded: {missing_vars_in_df}")

# Use only the available variables for the feature matrix X
X = df[available_vars_in_df].values
y = df['label'].values
weights = df['weight'].values


# Split randomly into train/test (50/50)
X_train, X_test, y_train, y_test, w_train, w_test = \
    train_test_split(X, y, weights, test_size=0.5, random_state=42, stratify=y)

#---------------------------------------------------
#import numpy as np

def renormalize_weights(y_part, w_part):
    # sum of weights per class in this partition
    sum_w_signal = w_part[y_part == 1].sum()
    sum_w_bkg    = w_part[y_part == 0].sum()
    # number of events per class
    n_signal = (y_part == 1).sum()
    n_bkg    = (y_part == 0).sum()
    # compute scaling factors
    scale_signal = n_signal / sum_w_signal
    scale_bkg    = n_bkg    / sum_w_bkg
    # apply
    w_scaled = w_part.copy()
    w_scaled[y_part == 1] *= scale_signal
    w_scaled[y_part == 0] *= scale_bkg
    return w_scaled

# Renormalize weights:
w_train_norm = renormalize_weights(y_train, w_train)
w_test_norm  = renormalize_weights(y_test,  w_test)
#---------------------------------------------------
#---------------------------------------------------

# === Check that sum of weights == number of events per class ===
# Signal in train
sum_signal_train = w_train_norm[y_train == 1].sum()
n_signal_train   = (y_train == 1).sum()
print(f"Signal (train): sum of weights = {sum_signal_train:.0f},   # events = {n_signal_train}")

# Background in train
sum_bkg_train  = w_train_norm[y_train == 0].sum()
n_bkg_train    = (y_train == 0).sum()
print(f"Background (train): sum of weights = {sum_bkg_train:.0f},   # events = {n_bkg_train}")

# You can do the same for the test set if you like:
sum_signal_test = w_test_norm[y_test == 1].sum()
n_signal_test   = (y_test == 1).sum()
print(f"Signal (test):  sum of weights = {sum_signal_test:.0f},   # events = {n_signal_test}")

sum_bkg_test  = w_test_norm[y_test == 0].sum()
n_bkg_test    = (y_test == 0).sum()
print(f"Background (test):  sum of weights = {sum_bkg_test:.0f},   # events = {n_bkg_test}")
#---------------------------------------------------
#---------------------------------------------------
sum(w_train_norm[y_train == 1]) == (y_train == 1).sum()
sum(w_train_norm[y_train == 0]) == (y_train == 0).sum()
#---------------------------------------------------
# Standardize features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test  = scaler.transform(X_test)

"""
# --- 2. Prepare training and testing datasets ---
X = df[variables].values
y = df['label'].values
weights = df['weight'].values

# Split randomly into train/test (50/50)
X_train, X_test, y_train, y_test, w_train, w_test = \
    train_test_split(X, y, weights, test_size=0.5, random_state=42, stratify=y)

# Standardize features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test  = scaler.transform(X_test)
"""

from xgboost import XGBClassifier

# --- 3. Define classifiers, now including a “TMVABDT” that matches the ROOT settings ---
classifiers = {
    'DecisionTree': DecisionTreeClassifier(
        max_depth=3,
        min_samples_leaf=int(0.025 * len(y_train))
    ),
    'RandomForest': RandomForestClassifier(
        n_estimators=100,
        max_depth=3
    ),
    'AdaBoost': AdaBoostClassifier(
        n_estimators=50
    ),
    'KNN': KNeighborsClassifier(
        n_neighbors=50
    ),
    'LDA': LinearDiscriminantAnalysis(),
    'SVM': SVC(
        kernel='rbf',
        probability=True
    ),
    'MLP': MLPClassifier(
        hidden_layer_sizes=(100,),
        max_iter=300
    ),
    'TMVABDT': XGBClassifier(
        n_estimators=850,         # NTrees
        max_depth=3,              # MaxDepth
        learning_rate=0.5,        # AdaBoostBeta
        subsample=0.5,            # BaggedSampleFraction
        objective='binary:logistic',
        use_label_encoder=False,
        eval_metric='logloss',
        verbosity=0
    )
}

'''
# Assuming you’ve already got:
# X_train, X_test, y_train, y_test, w_train_norm, w_test_norm (where X_train and X_test are already scaled)
# and your classifiers dict, e.g.:
# classifiers = { 'DecisionTree': DecisionTreeClassifier(...), …, 'TMVABDT': XGBClassifier(...) }

# Train each classifier with sample weights
trained_classifiers = {}
for name, clf in classifiers.items():
    print(f"Training {name}…")
    # Most estimators accept sample_weight directly
    clf.fit(
        X_train,  # Changed from X_train_scaled to X_train
        y_train,
        sample_weight=w_train_norm
    )
    trained_classifiers[name] = clf
    print(f"  → Done.")
    '''

'''
# --- 3. Define classifiers ---
classifiers = {
    'DecisionTree': DecisionTreeClassifier(max_depth=3, min_samples_leaf=int(0.025 * len(y_train))),
    'RandomForest': RandomForestClassifier(n_estimators=100, max_depth=3),
    'AdaBoost': AdaBoostClassifier(n_estimators=50),
    'KNN': KNeighborsClassifier(n_neighbors=50),
    'LDA': LinearDiscriminantAnalysis(),
    'SVM': SVC(kernel='rbf', probability=True),
    'MLP': MLPClassifier(hidden_layer_sizes=(100,), max_iter=300)
}
'''

'''
# --- 4. Train, evaluate and plot ROC curves ---
import inspect

plt.figure(figsize=(8,6))
for name, clf in classifiers.items():
    sig = inspect.signature(clf.fit)
    if "sample_weight" in sig.parameters:
        clf.fit(X_train, y_train, sample_weight=w_train)
    else:
        print(f"[Info] {name} does not support sample_weight; fitting without weights.")
        clf.fit(X_train, y_train)
    # ROC evaluation remains the same
    y_score = clf.predict_proba(X_test)[:,1]
    fpr, tpr, _ = roc_curve(y_test, y_score, sample_weight=w_test)
    roc_auc = auc(fpr, tpr)
    plt.plot(fpr, tpr, label=f"{name} (AUC = {roc_auc:.3f})")

plt.plot([0,1], [0,1], 'k--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curves for Multi-Classifiers')
plt.legend(loc='lower right')
plt.grid(True)
plt.savefig('roc_curves.png')
plt.show()
'''

import inspect


#import matplotlib.pyplot as plt
#from sklearn.metrics import roc_curve, auc

plt.figure(figsize=(8,6))

for name, clf in classifiers.items():
    sig = inspect.signature(clf.fit)
    if "sample_weight" in sig.parameters:
        # use normalized training weights
        clf.fit(X_train, y_train, sample_weight=w_train_norm)
    else:
        print(f"[Info] {name} does not support sample_weight; fitting without weights.")
        clf.fit(X_train, y_train)
    # ROC evaluation with normalized test weights
    y_score = clf.predict_proba(X_test)[:, 1]
    fpr, tpr, _ = roc_curve(y_test, y_score, sample_weight=w_test_norm)
    roc_auc = auc(fpr, tpr)
    plt.plot(fpr, tpr, label=f"{name} (AUC = {roc_auc:.3f})")

plt.plot([0,1], [0,1], 'k--', linewidth=1)
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curves for Multi-Classifiers')
plt.legend(loc='lower right', fontsize='small')
plt.grid(True)
plt.savefig('roc_curves.png', dpi=150)
plt.show()

"""
import inspect
#import matplotlib.pyplot as plt
#from sklearn.metrics import roc_curve, auc

plt.figure(figsize=(8,6))

for name, clf in classifiers.items():
    sig = inspect.signature(clf.fit)
    if "sample_weight" in sig.parameters:
        clf.fit(X_train, y_train, sample_weight=w_train_norm)
    else:
        print(f"[Info] {name} does not support sample_weight; fitting without weights.")
        clf.fit(X_train, y_train)
    y_score = clf.predict_proba(X_test)[:, 1]
    fpr, tpr, _ = roc_curve(y_test, y_score, sample_weight=w_test_norm)
    roc_auc = auc(fpr, tpr)
    # Swap FPR and TPR in the plot to flip axes
    plt.plot(tpr, fpr, label=f"{name} (AUC = {roc_auc:.3f}")  # TPR on x, FPR on y

plt.plot([0,1], [0,1], 'k--', linewidth=1)
plt.xlabel('True Positive Rate')  # Updated x-axis label
plt.ylabel('False Positive Rate')  # Updated y-axis label
plt.title('ROC Curves for Multi-Classifiers')
plt.legend(loc='lower right', fontsize='small')
plt.grid(True)
plt.savefig('roc_curves.png', dpi=150)
plt.show()
"""

# --- 5. Print accuracy scores ---
for name, clf in classifiers.items():
    y_pred = clf.predict(X_test)
    acc = accuracy_score(y_test,
                         y_pred,
                         #sample_weight=w_test
                         sample_weight=w_test_norm
                         )
    print(f"{name} accuracy: {acc:.3f}")